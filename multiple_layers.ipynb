{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50310004, -1.04184985, -2.03874993],\n",
       "       [-0.00999999, -2.51600027, -4.44200015],\n",
       "       [-0.99314   ,  1.41254002, -0.35655001]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 2.5], \n",
    "            [2., 5., -1., 2], \n",
    "            [-1.5, 2.7, 3.3, -0.8]] \n",
    "weights = [[0.2, 0.8, -0.5, 1], \n",
    "            [0.5, -0.91, 0.26, -0.5], \n",
    "            [-0.26, -0.27, 0.17, 0.87]] \n",
    "biases = [2, 3, 0.5] \n",
    "\n",
    "weights2 = [[0.1, -0.14, 0.5], \n",
    "           [-0.5, 0.12, -0.33], \n",
    "           [-0.44, 0.73, -0.13]] \n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "layer_1_output = np.dot(np.array(inputs),np.array(weights).T) + biases\n",
    "\n",
    "relu_output  = np.maximum(0,layer_1_output)\n",
    "\n",
    "\n",
    "layer_2_output = np.dot(relu_output,np.array(weights2).T) + biases2\n",
    "layer_2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data \n",
    "import nnfs \n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X, y = spiral_data(samples = 100, classes = 3) \n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='brg') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularizer_l1=0, weight_regularizer_l2=0, bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLu:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis= 1, keepdims=True))\n",
    "        \n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "    \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalue) in enumerate(zip(self.output, dvalues)):\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            \n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output,single_output.T)\n",
    "            \n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalue)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    def calculate(self,output,y):\n",
    "        sample_losses = self.forward(output,y)\n",
    "        \n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "    \n",
    "    def regularization_loss(self,layer):\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))  \n",
    "        \n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss +=  layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights) \n",
    "        \n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.bias))  \n",
    "        \n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss +=  layer.bias_regularizer_l2 * np.sum(layer.bias * layer.bias)  \n",
    "            \n",
    "        return regularization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1- 1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1: \n",
    "            correct_confidences = y_pred_clipped[ \n",
    "                range(samples), \n",
    "                y_true \n",
    "            ] \n",
    " \n",
    "        \n",
    "        elif len(y_true.shape) == 2: \n",
    "            correct_confidences = np.sum( \n",
    "                y_pred_clipped * y_true, \n",
    "                axis = 1 \n",
    "            ) \n",
    " \n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences) \n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "       \n",
    "        samples = len(dvalues) \n",
    "        \n",
    "        labels = len(dvalues[0]) \n",
    " \n",
    "        \n",
    "        if len(y_true.shape) == 1: \n",
    "            y_true = np.eye(labels)[y_true] \n",
    " \n",
    "        \n",
    "        self.dinputs = -y_true / dvalues \n",
    "        \n",
    "        self.dinputs = self.dinputs / samples \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Acuracy:\n",
    "    \n",
    "    def calculate(self, inputs, y_true):\n",
    "        \n",
    "        predictions = np.argmax(inputs, axis=1)\n",
    "        \n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        acuracy = np.mean(predictions==y_true)\n",
    "        return acuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        if (len(y_true.shape)==2):\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SDG:\n",
    "    \n",
    "    def __init__(self,learning_rate=1, learning_rate_decay = 0, momentum = 0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.steps = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        # Update current learning rate if there is a decay function\n",
    "        if self.learning_rate_decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1/ (1+self.learning_rate_decay*self.steps))\n",
    "        \n",
    "    def update_params(self,layer):\n",
    "        if self.momentum:\n",
    "            \n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                # Store momentums in the layer\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                \n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Update with momentums\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        \n",
    "        else:\n",
    "            # Normal update\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "        layer.weights +=  weight_updates\n",
    "        layer.biases +=  bias_updates\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_AdaGrad:\n",
    "    \n",
    "    def __init__(self,learning_rate=1, learning_rate_decay = 0,epsilon = 1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.steps = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        # Update current learning rate if there is a decay function\n",
    "        if self.learning_rate_decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1/ (1+self.learning_rate_decay*self.steps))\n",
    "        \n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            # Store caches in the layer\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "                \n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases **2\n",
    "        \n",
    "        layer.weights +=  -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases +=  -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "    \n",
    "    def __init__(self,learning_rate=0.001, learning_rate_decay = 0,epsilon = 1e-7, rho = 0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.steps = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        # Update current learning rate if there is a decay function\n",
    "        if self.learning_rate_decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1/ (1+self.learning_rate_decay*self.steps))\n",
    "        \n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            # Store caches in the layer\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases ** 2\n",
    "        \n",
    "        layer.weights +=  -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases +=  -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    \n",
    "    def __init__(self,learning_rate=0.001, learning_rate_decay = 0,epsilon = 1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.steps = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        # Update current learning rate if there is a decay function\n",
    "        if self.learning_rate_decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1/ (1+self.learning_rate_decay*self.steps))\n",
    "        \n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            # Store momentums and caches in the layer\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            \n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update momentums\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1-self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1-self.beta_1) * layer.dbiases\n",
    "        \n",
    "        # Corrected momentums (1 - b_1^step)\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.steps + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.steps + 1))\n",
    "        \n",
    "        # Update Cache\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases ** 2\n",
    "        \n",
    "        # Corrected cache (1 - b_2^step)\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.steps + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.steps + 1))\n",
    "        \n",
    "        layer.weights +=  -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases +=  -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    \n",
    "        def __init__(self, rate):\n",
    "            # We will be using droput rate and invert for success rate\n",
    "            self.rate = 1 - rate\n",
    "        \n",
    "        def forward(self, inputs):\n",
    "            self.inputs = inputs\n",
    "            \n",
    "            self.binary_mask = np.random.binomial(1, self.rate, size = inputs.shape) / self.rate\n",
    "            \n",
    "            self.output = inputs * self.binary_mask\n",
    "        \n",
    "        def backward(self, dvalues):\n",
    "            self.dinputs = self.binary_mask * dvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.3333333  0.33333334]\n",
      " [0.3333335  0.333333   0.33333352]\n",
      " [0.33333334 0.33333334 0.33333334]]\n",
      "1.0986118\n",
      "0.33666666666666667\n"
     ]
    }
   ],
   "source": [
    "#Data\n",
    "X, y = spiral_data(samples = 100, classes = 3) \n",
    "\n",
    "#First layer with 2 inputs and 3 neurons\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "#ReLu for first layer\n",
    "activation1 = Activation_ReLu()\n",
    "\n",
    "#Second layer with 3 inputs(As it uses the outputs from the previous layer) and 3 neurons\n",
    "dense2 = Layer_Dense(3,3)\n",
    "\n",
    "#SoftMax for the second layer and for the output\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function \n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create acuracy function \n",
    "acuracy_function = Acuracy()\n",
    "\n",
    "#Forward of our data on the first layer\n",
    "dense1.forward(X)\n",
    "\n",
    "#ReLu activation of the data in the first layer\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "#Forward of our outputs fron the ReLu of the first layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "#SoftMax activation of the data in the first layer\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])\n",
    "\n",
    "#Loss function of the data of the softmax\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "print(loss)\n",
    "\n",
    "#Acuracy function of the data of the softmax\n",
    "acuracy = acuracy_function.calculate(activation2.output, y)\n",
    "print(acuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.337, loss: 1.099, lr: 1\n",
      "epoch: 100, acc: 0.430, loss: 1.068, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.417, loss: 1.004, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.563, loss: 0.845, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.613, loss: 0.761, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.643, loss: 0.714, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.670, loss: 0.690, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.657, loss: 0.676, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.667, loss: 0.667, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.677, loss: 0.659, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.677, loss: 0.652, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.673, loss: 0.646, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.677, loss: 0.641, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.677, loss: 0.606, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.750, loss: 0.524, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.780, loss: 0.506, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.790, loss: 0.496, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.787, loss: 0.490, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.777, loss: 0.486, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.783, loss: 0.476, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.817, loss: 0.398, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.827, loss: 0.377, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.830, loss: 0.368, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.833, loss: 0.361, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.837, loss: 0.357, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.837, loss: 0.352, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.840, loss: 0.348, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.847, loss: 0.345, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.850, loss: 0.340, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.853, loss: 0.334, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.850, loss: 0.331, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.843, loss: 0.328, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.843, loss: 0.325, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.840, loss: 0.323, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.843, loss: 0.320, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.850, loss: 0.318, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.843, loss: 0.316, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.860, loss: 0.314, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.860, loss: 0.312, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.857, loss: 0.311, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.860, loss: 0.309, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.863, loss: 0.308, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.863, loss: 0.307, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.863, loss: 0.306, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.863, loss: 0.304, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.863, loss: 0.303, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.860, loss: 0.302, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.860, loss: 0.302, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.860, loss: 0.301, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.860, loss: 0.300, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.860, loss: 0.300, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.860, loss: 0.299, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.860, loss: 0.299, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.860, loss: 0.298, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.860, loss: 0.298, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.860, loss: 0.297, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.860, loss: 0.297, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.860, loss: 0.296, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.863, loss: 0.296, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.873, loss: 0.295, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.870, loss: 0.295, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.873, loss: 0.294, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.870, loss: 0.294, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.870, loss: 0.293, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.870, loss: 0.293, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.870, loss: 0.292, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.870, loss: 0.292, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.873, loss: 0.292, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.870, loss: 0.292, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.870, loss: 0.291, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.873, loss: 0.291, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.877, loss: 0.290, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.873, loss: 0.290, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.877, loss: 0.290, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.877, loss: 0.290, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.880, loss: 0.289, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.877, loss: 0.289, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.877, loss: 0.289, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.880, loss: 0.289, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.877, loss: 0.289, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.880, loss: 0.288, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.883, loss: 0.288, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.880, loss: 0.288, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.877, loss: 0.288, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.883, loss: 0.288, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.880, loss: 0.287, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.883, loss: 0.287, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.883, loss: 0.287, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.883, loss: 0.287, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.883, loss: 0.287, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.883, loss: 0.286, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.883, loss: 0.286, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.883, loss: 0.286, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.880, loss: 0.286, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.883, loss: 0.286, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.880, loss: 0.286, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.880, loss: 0.285, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.883, loss: 0.285, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.883, loss: 0.285, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.883, loss: 0.285, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.883, loss: 0.285, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "#Data\n",
    "X, y = spiral_data(samples = 100, classes = 3) \n",
    "\n",
    "#First layer with 2 inputs and 64 neurons\n",
    "dense1 = Layer_Dense(2,64)\n",
    "\n",
    "#ReLu for first layer\n",
    "activation1 = Activation_ReLu()\n",
    "\n",
    "#Second layer with 64 inputs(As it uses the outputs from the previous layer) and 3 neurons\n",
    "dense2 = Layer_Dense(64,3)\n",
    "\n",
    "#Softmax with loss for backpropagation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "#SDG Optimizer\n",
    "optimizer = Optimizer_SDG(learning_rate_decay=1e-3,momentum=.9)\n",
    "\n",
    "#Create acuracy function \n",
    "acuracy_function = Acuracy()\n",
    "\n",
    "for epoch in range(10001):\n",
    "    #Forward of our data on the first layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    #ReLu activation of the data in the first layer\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    #Forward of our outputs fron the ReLu of the first layer\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    #SoftMax activation of the data in the first layer\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    #Acuracy function of the data of the softmax\n",
    "    accuracy = acuracy_function.calculate(loss_activation.output, y)\n",
    "\n",
    "    \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}') \n",
    "\n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    "\n",
    "    #Using sdg to update params\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.310, loss: 1.099, lr: 0.05\n",
      "epoch: 100, acc: 0.630, loss: 0.834, lr: 0.04999752512250644\n",
      "epoch: 200, acc: 0.730, loss: 0.678, lr: 0.04999502549496326\n",
      "epoch: 300, acc: 0.763, loss: 0.573, lr: 0.049992526117345455\n",
      "epoch: 400, acc: 0.810, loss: 0.501, lr: 0.04999002698961558\n",
      "epoch: 500, acc: 0.790, loss: 0.477, lr: 0.049987528111736124\n",
      "epoch: 600, acc: 0.837, loss: 0.427, lr: 0.049985029483669646\n",
      "epoch: 700, acc: 0.853, loss: 0.404, lr: 0.049982531105378675\n",
      "epoch: 800, acc: 0.860, loss: 0.380, lr: 0.04998003297682575\n",
      "epoch: 900, acc: 0.863, loss: 0.364, lr: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.870, loss: 0.347, lr: 0.049975037468784345\n",
      "epoch: 1100, acc: 0.883, loss: 0.342, lr: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.887, loss: 0.324, lr: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.867, loss: 0.330, lr: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.900, loss: 0.295, lr: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.913, loss: 0.287, lr: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.917, loss: 0.282, lr: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.890, loss: 0.280, lr: 0.04995756105188642\n",
      "epoch: 1800, acc: 0.923, loss: 0.263, lr: 0.049955065418655915\n",
      "epoch: 1900, acc: 0.907, loss: 0.255, lr: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.900, loss: 0.262, lr: 0.04995007490013731\n",
      "epoch: 2100, acc: 0.903, loss: 0.247, lr: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.923, loss: 0.233, lr: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.897, loss: 0.243, lr: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.903, loss: 0.237, lr: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.923, loss: 0.225, lr: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.917, loss: 0.223, lr: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.927, loss: 0.215, lr: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.927, loss: 0.209, lr: 0.04993012279315098\n",
      "epoch: 2900, acc: 0.923, loss: 0.208, lr: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.930, loss: 0.205, lr: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.910, loss: 0.231, lr: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.933, loss: 0.198, lr: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.933, loss: 0.195, lr: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.933, loss: 0.194, lr: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.923, loss: 0.195, lr: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.947, loss: 0.188, lr: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.947, loss: 0.183, lr: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.930, loss: 0.179, lr: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.937, loss: 0.179, lr: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.950, loss: 0.172, lr: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.950, loss: 0.169, lr: 0.04989773459295174\n",
      "epoch: 4200, acc: 0.947, loss: 0.166, lr: 0.04989524493326262\n",
      "epoch: 4300, acc: 0.947, loss: 0.164, lr: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.940, loss: 0.171, lr: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.947, loss: 0.162, lr: 0.04988777744463829\n",
      "epoch: 4600, acc: 0.950, loss: 0.159, lr: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.933, loss: 0.174, lr: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.943, loss: 0.155, lr: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.933, loss: 0.173, lr: 0.049877824269451976\n",
      "epoch: 5000, acc: 0.947, loss: 0.155, lr: 0.04987533659617785\n",
      "epoch: 5100, acc: 0.943, loss: 0.148, lr: 0.04987284917103844\n",
      "epoch: 5200, acc: 0.943, loss: 0.163, lr: 0.04987036199399661\n",
      "epoch: 5300, acc: 0.950, loss: 0.150, lr: 0.04986787506501525\n",
      "epoch: 5400, acc: 0.677, loss: 1.694, lr: 0.04986538838405724\n",
      "epoch: 5500, acc: 0.943, loss: 0.146, lr: 0.049862901951085496\n",
      "epoch: 5600, acc: 0.943, loss: 0.142, lr: 0.049860415766062906\n",
      "epoch: 5700, acc: 0.943, loss: 0.141, lr: 0.0498579298289524\n",
      "epoch: 5800, acc: 0.943, loss: 0.140, lr: 0.04985544413971689\n",
      "epoch: 5900, acc: 0.943, loss: 0.139, lr: 0.049852958698319315\n",
      "epoch: 6000, acc: 0.943, loss: 0.138, lr: 0.04985047350472258\n",
      "epoch: 6100, acc: 0.943, loss: 0.138, lr: 0.04984798855888967\n",
      "epoch: 6200, acc: 0.943, loss: 0.137, lr: 0.049845503860783506\n",
      "epoch: 6300, acc: 0.957, loss: 0.138, lr: 0.049843019410367055\n",
      "epoch: 6400, acc: 0.947, loss: 0.135, lr: 0.04984053520760327\n",
      "epoch: 6500, acc: 0.947, loss: 0.135, lr: 0.049838051252455155\n",
      "epoch: 6600, acc: 0.943, loss: 0.146, lr: 0.049835567544885655\n",
      "epoch: 6700, acc: 0.950, loss: 0.133, lr: 0.04983308408485778\n",
      "epoch: 6800, acc: 0.953, loss: 0.137, lr: 0.0498306008723345\n",
      "epoch: 6900, acc: 0.943, loss: 0.132, lr: 0.04982811790727884\n",
      "epoch: 7000, acc: 0.953, loss: 0.140, lr: 0.04982563518965381\n",
      "epoch: 7100, acc: 0.943, loss: 0.131, lr: 0.049823152719422406\n",
      "epoch: 7200, acc: 0.953, loss: 0.141, lr: 0.049820670496547675\n",
      "epoch: 7300, acc: 0.950, loss: 0.130, lr: 0.04981818852099264\n",
      "epoch: 7400, acc: 0.957, loss: 0.137, lr: 0.049815706792720335\n",
      "epoch: 7500, acc: 0.950, loss: 0.130, lr: 0.0498132253116938\n",
      "epoch: 7600, acc: 0.953, loss: 0.141, lr: 0.04981074407787611\n",
      "epoch: 7700, acc: 0.950, loss: 0.128, lr: 0.049808263091230306\n",
      "epoch: 7800, acc: 0.950, loss: 0.126, lr: 0.04980578235171948\n",
      "epoch: 7900, acc: 0.943, loss: 0.138, lr: 0.04980330185930667\n",
      "epoch: 8000, acc: 0.953, loss: 0.137, lr: 0.04980082161395499\n",
      "epoch: 8100, acc: 0.950, loss: 0.127, lr: 0.04979834161562752\n",
      "epoch: 8200, acc: 0.957, loss: 0.126, lr: 0.04979586186428736\n",
      "epoch: 8300, acc: 0.953, loss: 0.124, lr: 0.04979338235989761\n",
      "epoch: 8400, acc: 0.947, loss: 0.133, lr: 0.04979090310242139\n",
      "epoch: 8500, acc: 0.953, loss: 0.123, lr: 0.049788424091821805\n",
      "epoch: 8600, acc: 0.953, loss: 0.122, lr: 0.049785945328062006\n",
      "epoch: 8700, acc: 0.960, loss: 0.127, lr: 0.0497834668111051\n",
      "epoch: 8800, acc: 0.953, loss: 0.127, lr: 0.049780988540914256\n",
      "epoch: 8900, acc: 0.953, loss: 0.130, lr: 0.0497785105174526\n",
      "epoch: 9000, acc: 0.953, loss: 0.127, lr: 0.04977603274068329\n",
      "epoch: 9100, acc: 0.947, loss: 0.133, lr: 0.04977355521056952\n",
      "epoch: 9200, acc: 0.950, loss: 0.135, lr: 0.049771077927074414\n",
      "epoch: 9300, acc: 0.953, loss: 0.119, lr: 0.0497686008901612\n",
      "epoch: 9400, acc: 0.963, loss: 0.122, lr: 0.04976612409979302\n",
      "epoch: 9500, acc: 0.953, loss: 0.122, lr: 0.0497636475559331\n",
      "epoch: 9600, acc: 0.953, loss: 0.120, lr: 0.049761171258544616\n",
      "epoch: 9700, acc: 0.953, loss: 0.125, lr: 0.0497586952075908\n",
      "epoch: 9800, acc: 0.963, loss: 0.120, lr: 0.04975621940303483\n",
      "epoch: 9900, acc: 0.953, loss: 0.130, lr: 0.049753743844839965\n",
      "epoch: 10000, acc: 0.953, loss: 0.121, lr: 0.04975126853296942\n"
     ]
    }
   ],
   "source": [
    "#Data\n",
    "X, y = spiral_data(samples = 100, classes = 3) \n",
    "\n",
    "#First layer with 2 inputs and 64 neurons\n",
    "dense1 = Layer_Dense(2,64)\n",
    "\n",
    "#ReLu for first layer\n",
    "activation1 = Activation_ReLu()\n",
    "\n",
    "dropout1 = Layer_Dropout(0.1) \n",
    "\n",
    "#Second layer with 64 inputs(As it uses the outputs from the previous layer) and 3 neurons\n",
    "dense2 = Layer_Dense(64,3)\n",
    "\n",
    "#Softmax with loss for backpropagation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "#SDG Optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, learning_rate_decay=5e-7)\n",
    "\n",
    "#Create acuracy function \n",
    "acuracy_function = Acuracy()\n",
    "\n",
    "for epoch in range(10001):\n",
    "    #Forward of our data on the first layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    #ReLu activation of the data in the first layer\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    #Forward of our outputs fron the ReLu of the first layer\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    #SoftMax activation of the data in the first layer\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    #Acuracy function of the data of the softmax\n",
    "    accuracy = acuracy_function.calculate(loss_activation.output, y)\n",
    "\n",
    "    \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}') \n",
    "\n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    "\n",
    "    #Using sdg to update params\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n",
      "[[0.8]\n",
      " [0.5]\n",
      " [0.9]]\n",
      "[0.8 0.5 0.9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34055041584399376"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "softmax_outputs = np.array([[0.8, 0.1, 0.2], \n",
    "                            [0.1, 0.5, 0.4], \n",
    "                            [0.02, 0.9, 0.08]]) \n",
    "class_targets = np.array([[1, 0, 0], \n",
    "                          [0, 1, 0], \n",
    "                          [0, 1, 0]]) \n",
    "\n",
    "print(np.argmax(class_targets, axis=1))\n",
    "print(np.max(softmax_outputs, axis= 1, keepdims=True))\n",
    "\n",
    "correct_confidences = np.sum( \n",
    "    softmax_outputs * class_targets, \n",
    "    axis = 1\n",
    "    )\n",
    "print(correct_confidences)\n",
    "pred = -np.log(correct_confidences)\n",
    "np.mean(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6]]\n",
      "[[1.         0.00673795 0.00673795]]\n",
      "[[1.01347589]]\n",
      "[[0.98670329 0.00664835 0.00664835]]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[6,1,1]])\n",
    "\n",
    "exp_values = np.exp(inputs - np.max(inputs, axis= 1, keepdims=True))\n",
    "print(np.max(inputs, axis= 1, keepdims=True))\n",
    "print(exp_values)\n",
    "print(np.sum(exp_values, axis=1,keepdims=True))\n",
    "        \n",
    "probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "print(probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
